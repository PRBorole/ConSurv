{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc68e4e-db8e-49a9-abdb-be8ea37d939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "\n",
    "from rulekit.classification import RuleClassifier\n",
    "from rulekit.params import Measures\n",
    "\n",
    "from rulekit.survival import SurvivalRules\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "sys.path.append('./../src/')\n",
    "from utils import *\n",
    "\n",
    "_ = RuleClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22201d01-7443-42f5-b66c-dd83ba10c73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    921\n",
       "1    246\n",
       "2     42\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('./../Data/1000_features_survival_3classes.csv',\n",
    "                      index_col=0).drop(['index'],axis=1)\n",
    "data_df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff364da-4faf-480e-9151-7b94eb6451a0",
   "metadata": {},
   "source": [
    "# Best configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d38e17a-c2a3-492f-8f4b-32054a9a62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = Measures.RSS\n",
    "minsupp_new = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3a0b9a-6606-4345-ac8b-6be3a1a8675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "nclasses = 3\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Loop through each column and apply encoding to object type columns\n",
    "for col in data_df.columns:\n",
    "    if data_df[col].dtype == 'object':\n",
    "        data_df[col] = le.fit_transform(data_df[col])\n",
    "\n",
    "data_df = data_df.fillna(data_df.mean())\n",
    "\n",
    "X = data_df.drop(['event', 'time','y'], axis=1)\n",
    "y = data_df['y']\n",
    "\n",
    "\n",
    "# split data into train and test sets\n",
    "seeds = [999, 7, 42, 1995, 1303, 2405, 1996, 200, 0, 777]\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1116deb3-54d8-4763-a451-0df146e27405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training:  115.15857529640198\n",
      "*********************** TRAIN ***********************\n",
      "accuracy:  0.9988179669030733\n",
      "MCC:  0.9968688567089448\n",
      "f1:  [0.9992242  0.99710145 1.        ]\n",
      "auroc:  [0.99999229 0.99999137 1.        ]\n",
      "auprc:  [0.9999975981587497, 0.9999662956386361, 1.0]\n",
      "*********************** VALID ***********************\n",
      "accuracy:  0.7569060773480663\n",
      "MCC:  0.24544095898566085\n",
      "f1:  [0.85324232 0.38095238 0.        ]\n",
      "auroc:  [0.63506909 0.65915916 0.54428571]\n",
      "auprc:  [0.8577759258677471, 0.34936521755617367, 0.0799886252843679]\n",
      "*********************** TEST ***********************\n",
      "accuracy:  0.7582417582417582\n",
      "MCC:  0.25326513001785\n",
      "f1:  [0.85423729 0.38709677 0.        ]\n",
      "auroc:  [0.6240942  0.65964585 0.52897959]\n",
      "auprc:  [0.8518193388199491, 0.3475407657849425, 0.05219780219780221]\n",
      "Number of rules:  63\n",
      "Time for training:  108.46986484527588\n",
      "*********************** TRAIN ***********************\n",
      "accuracy:  0.9988179669030733\n",
      "MCC:  0.9968688567089448\n",
      "f1:  [0.9992242  0.99710145 1.        ]\n",
      "auroc:  [1. 1. 1.]\n",
      "auprc:  [1.0, 1.0, 1.0]\n",
      "*********************** VALID ***********************\n",
      "accuracy:  0.7182320441988951\n",
      "MCC:  0.15470540489023718\n",
      "f1:  [0.82352941 0.33846154 0.        ]\n",
      "auroc:  [0.57566566 0.58323949 0.4947619 ]\n",
      "auprc:  [0.8304232675462098, 0.36413292926087226, 0.03163897982346706]\n",
      "*********************** TEST ***********************\n",
      "accuracy:  0.7087912087912088\n",
      "MCC:  0.055277436800890416\n",
      "f1:  [0.82274247 0.21052632 0.        ]\n",
      "auroc:  [0.59906126 0.63112768 0.45142857]\n",
      "auprc:  [0.8430247172666103, 0.3101239069968302, 0.019230769230769232]\n",
      "Number of rules:  66\n",
      "Time for training:  129.9514663219452\n",
      "*********************** TRAIN ***********************\n",
      "accuracy:  0.9952718676122931\n",
      "MCC:  0.9874674095045498\n",
      "f1:  [0.99767261 0.98837209 0.98305085]\n",
      "auroc:  [0.99998457 0.99993099 0.99995779]\n",
      "auprc:  [0.9999951925850432, 0.9997360617125824, 0.9988307570352755]\n",
      "*********************** VALID ***********************\n",
      "accuracy:  0.712707182320442\n",
      "MCC:  0.09430526364535842\n",
      "f1:  [0.82312925 0.2295082  0.28571429]\n",
      "auroc:  [0.62428379 0.59647147 0.71380952]\n",
      "auprc:  [0.8512887028727412, 0.26900558792155893, 0.31609753812792485]\n",
      "*********************** TEST ***********************\n",
      "accuracy:  0.6978021978021978\n",
      "MCC:  0.08745182305084181\n",
      "f1:  [0.81099656 0.27272727 0.        ]\n",
      "auroc:  [0.61758893 0.61780056 0.50285714]\n",
      "auprc:  [0.8470244799053139, 0.26541229535283106, 0.039210789210789215]\n",
      "Number of rules:  73\n",
      "Time for training:  142.37168622016907\n",
      "*********************** TRAIN ***********************\n",
      "accuracy:  0.9940898345153665\n",
      "MCC:  0.9844342329927309\n",
      "f1:  [0.997669   0.98550725 0.96666667]\n",
      "auroc:  [0.99998457 0.99990511 1.        ]\n",
      "auprc:  [0.9999952037477982, 0.9996345395090462, 1.0]\n",
      "*********************** VALID ***********************\n",
      "accuracy:  0.7071823204419889\n",
      "MCC:  0.07589183029065522\n",
      "f1:  [0.82312925 0.2295082  0.        ]\n",
      "auroc:  [0.49721941 0.48001126 0.50190476]\n",
      "auprc:  [0.8062334428337514, 0.19539269224774933, 0.033653424537402436]\n",
      "*********************** TEST ***********************\n",
      "accuracy:  0.7197802197802198\n",
      "MCC:  0.1845185741793041\n",
      "f1:  [0.82926829 0.34285714 0.        ]\n",
      "auroc:  [0.65769104 0.6695247  0.58      ]\n",
      "auprc:  [0.8645665834061251, 0.332867887191868, 0.07275641025641026]\n",
      "Number of rules:  83\n"
     ]
    }
   ],
   "source": [
    "result = {'seed': seeds,\n",
    "          'measure': ['rss']*len(seeds),\n",
    "          'minsupp_new': [3]*len(seeds),\n",
    "          'time': [None]*len(seeds),\n",
    "          'accuracy_train': [None]*len(seeds),\n",
    "          'MCC_train': [None]*len(seeds),\n",
    "          'f1_train': [None]*len(seeds), \n",
    "          'f1_train': [None]*len(seeds), \n",
    "          'auroc_train': [None]*len(seeds),\n",
    "          'auprc_train': [None]*len(seeds),\n",
    "          'accuracy_val': [None]*len(seeds),\n",
    "          'MCC_val': [None]*len(seeds),\n",
    "          'f1_val': [None]*len(seeds), \n",
    "          'auroc_val': [None]*len(seeds),\n",
    "          'auprc_val': [None]*len(seeds),\n",
    "          'accuracy_test': [None]*len(seeds),\n",
    "          'MCC_test': [None]*len(seeds),\n",
    "          'f1_test': [None]*len(seeds), \n",
    "          'auroc_test': [None]*len(seeds),\n",
    "          'auprc_test': [None]*len(seeds),\n",
    "          'nrules': [None]*len(seeds),\n",
    "          'rules_count': [None]*len(seeds),\n",
    "          'conditions_per_rule': [None]*len(seeds),\n",
    "          'induced_conditions_per_rule': [None]*len(seeds),\n",
    "          'avg_rule_coverage': [None]*len(seeds),\n",
    "          'avg_rule_precision': [None]*len(seeds),\n",
    "          'avg_rule_quality': [None]*len(seeds),\n",
    "          'pvalue': [None]*len(seeds)}\n",
    "\n",
    "for idx, seed in enumerate(seeds):\n",
    "    # split data into train and test sets\n",
    "    X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=test_size, stratify=y, random_state=seed)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=seed)\n",
    "\n",
    "    \n",
    "    classifier = RuleClassifier(induction_measure=measure,\n",
    "                                pruning_measure=measure,\n",
    "                                voting_measure=measure,\n",
    "                                minsupp_new=minsupp_new)\n",
    "    \n",
    "    start = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    elapsed_time = end-start\n",
    "    print(\"Time for training: \", elapsed_time)\n",
    "    result['time'][idx] = [elapsed_time]\n",
    "    ruleset = classifier.model\n",
    "    \n",
    "    mapper_dict = {class_:idx for idx, class_ in enumerate(classifier.label_unique_values)}\n",
    "\n",
    "    # make predictions for train data\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_train_proba = classifier.predict_proba(X_train)\n",
    "    \n",
    "    ## need this correction because rulekit changes classes order randomly (maybe based on number of instances)\n",
    "    y_train_proba_corrected = [None]*nclasses\n",
    "    for class_ in range(nclasses):\n",
    "        y_train_proba_corrected[class_] =  y_train_proba[:,mapper_dict[class_]]\n",
    "    y_train_proba_corrected = np.array(y_train_proba_corrected).T\n",
    "    \n",
    "    # make predictions for val data\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    y_val_proba = classifier.predict_proba(X_val)\n",
    "    \n",
    "    ## need this correction because rulekit changes classes order randomly (maybe based on number of instances)\n",
    "    y_val_proba_corrected = [None]*nclasses\n",
    "    for class_ in range(nclasses):\n",
    "        y_val_proba_corrected[class_] =  y_val_proba[:,mapper_dict[class_]]\n",
    "    y_val_proba_corrected = np.array(y_val_proba_corrected).T\n",
    "    \n",
    "    # make predictions for test data\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    y_test_proba = classifier.predict_proba(X_test)\n",
    "    \n",
    "    ## need this correction because rulekit changes classes order randomly (maybe based on number of instances)\n",
    "    y_test_proba_corrected = [None]*nclasses\n",
    "    for class_ in range(nclasses):\n",
    "        y_test_proba_corrected[class_] =  y_test_proba[:,mapper_dict[class_]]\n",
    "    y_test_proba_corrected = np.array(y_test_proba_corrected).T\n",
    "    \n",
    "    # make binary labels\n",
    "    label_binarizer = LabelBinarizer().fit(y_train)\n",
    "\n",
    "    ################### RESULTS\n",
    "    ## Train\n",
    "    print(\"*********************** TRAIN ***********************\")\n",
    "    y_onehot_train = label_binarizer.transform(y_train)\n",
    "    accuracy, MCC, f1, auroc, auprc = get_metrics(y_train, y_train_pred, y_train_proba_corrected, y_onehot_train)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"MCC: \", MCC)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"auroc: \", auroc)\n",
    "    print(\"auprc: \", auprc)\n",
    "    result['accuracy_train'][idx] = accuracy\n",
    "    result['MCC_train'][idx] = MCC\n",
    "    result['f1_train'][idx] = f1\n",
    "    result['auroc_train'][idx] = auroc\n",
    "    result['auprc_train'][idx] = auprc\n",
    "        \n",
    "    ## valid\n",
    "    y_onehot_val = label_binarizer.transform(y_val)\n",
    "    print(\"*********************** VALID ***********************\")\n",
    "    accuracy, MCC, f1, auroc, auprc = get_metrics(y_val, y_val_pred, y_val_proba_corrected, y_onehot_val)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"MCC: \", MCC)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"auroc: \", auroc)\n",
    "    print(\"auprc: \", auprc)\n",
    "    result['accuracy_val'][idx] = accuracy\n",
    "    result['MCC_val'][idx] = MCC\n",
    "    result['f1_val'][idx] = f1\n",
    "    result['auroc_val'][idx] = auroc\n",
    "    result['auprc_val'][idx] = auprc\n",
    "    \n",
    "    ## test\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    print(\"*********************** TEST ***********************\")\n",
    "    accuracy, MCC, f1, auroc, auprc = get_metrics(y_test, y_test_pred, y_test_proba_corrected, y_onehot_test)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"MCC: \", MCC)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"auroc: \", auroc)\n",
    "    print(\"auprc: \", auprc)\n",
    "    result['accuracy_test'][idx] = accuracy\n",
    "    result['MCC_test'][idx] = MCC\n",
    "    result['f1_test'][idx] = f1\n",
    "    result['auroc_test'][idx] = auroc\n",
    "    result['auprc_test'][idx] = auprc\n",
    "    \n",
    "    print(\"Number of rules: \", len(ruleset.rules))\n",
    "    result['nrules'][idx] = len(ruleset.rules)\n",
    "    \n",
    "    # Get the stats\n",
    "    tmp_dict = vars(classifier.model.stats)\n",
    "    result['rules_count'][idx] = tmp_dict['rules_count']\n",
    "    result['conditions_per_rule'][idx] = tmp_dict['conditions_per_rule']\n",
    "    result['induced_conditions_per_rule'][idx] = tmp_dict['induced_conditions_per_rule']\n",
    "    result['avg_rule_coverage'][idx] = tmp_dict['avg_rule_coverage']\n",
    "    result['avg_rule_precision'][idx] = tmp_dict['avg_rule_precision']\n",
    "    result['avg_rule_quality'][idx] = tmp_dict['avg_rule_quality']\n",
    "    result['pvalue'][idx] = tmp_dict['pvalue']\n",
    "    \n",
    "    # rules_df = pd.DataFrame({'rules':[str(rule) for rule in ruleset.rules if 'y = {2}' not in str(rule)]})\n",
    "    rules_df = pd.DataFrame({'rules':[str(rule) for rule in ruleset.rules]})\n",
    "    rules_df['conditions'] = [[condition.split(\" = \")[0] for condition in re.sub(\"IF \", \"\", rule).split(\" THEN \")[0].split(' AND ')] \n",
    "                              for rule in rules_df['rules']]\n",
    "    rules_df['nconditions'] = [rule.count('AND')+1 for rule in rules_df['rules']]\n",
    "    rules_df.to_csv('./../results/RuleKit/csvs/classes3/rules_class_seed'+str(seed)+'.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928b74f-6f44-45de-a1de-3e3b796b987b",
   "metadata": {},
   "source": [
    "# Results for rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c377bb03-1d5c-4f7b-bf9c-16c26633194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min, max rule counts 60 83\n",
      "Min, max, median conditions counts 8 58 21.0\n",
      "% of class 0:  0.7405329593267882\n",
      "% of class 1:  0.20144927536231885\n",
      "% of class 2:  0.05334281650071124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeds = [999, 7, 42, 1995, 1303, 2405, 1996, 200, 0, 777]\n",
    "rules_df = []\n",
    "for seed in seeds:\n",
    "    rules_df_ = pd.read_csv('./../results/RuleKit/rules_class_seed'+str(seed)+'.csv',index_col=0)\n",
    "    rules_df_['seed'] = [seed]*len(rules_df_)\n",
    "    rules_df = rules_df + [rules_df_]\n",
    "rules_df = pd.concat(rules_df)\n",
    "\n",
    "# Get rules min max conditions etc\n",
    "print(\"Min, max rule counts\", rules_df.value_counts('seed').min(), rules_df.value_counts('seed').max())\n",
    "print(\"Min, max, median conditions counts\", rules_df['nconditions'].min(), rules_df['nconditions'].max(), rules_df['nconditions'].median())\n",
    "\n",
    "# Get rules class distribution\n",
    "rules_df['class'] = rules_df[\"rules\"].apply(lambda x: x[-2:-1])\n",
    "df1 = rules_df.value_counts(['seed','class']).reset_index()\n",
    "df2 = rules_df.value_counts(['seed']).reset_index()\n",
    "rule_dist_df = df1.merge(df2, on='seed')\n",
    "rule_dist_df['percent'] = rule_dist_df['count_x']/rule_dist_df['count_y']\n",
    "print(\"% of class 0: \", rule_dist_df[rule_dist_df['class']=='0']['percent'].median())\n",
    "print(\"% of class 1: \", rule_dist_df[rule_dist_df['class']=='1']['percent'].median())\n",
    "print(\"% of class 2: \", rule_dist_df[rule_dist_df['class']=='2']['percent'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1533f05-d7c2-445c-8430-2b3a446ec0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
