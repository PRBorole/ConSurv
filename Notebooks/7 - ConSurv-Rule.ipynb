{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917a93f1-a62d-491b-ad19-e47e8c96edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "from torchsurv.loss import cox\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "sys.path.append('./../src/')\n",
    "from utils import *\n",
    "from utils_RuleMLP import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424d1f6-c609-4e1b-b4b5-fa4794256968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708cc6dd-51fc-4bc2-869c-2f98fff955f5",
   "metadata": {},
   "source": [
    "# Best configs run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255338b3-3cdb-40a8-81a8-b871f6e6176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "seeds = [999, 7, 42, 1995, 1303, 2405, 1996, 200, 0, 777]\n",
    "test_size = 0.3\n",
    "batch_size = 64\n",
    "\n",
    "# hidden_size = 128  # Number of neurons in the hidden layers\n",
    "l2_reg = 0.001\n",
    "lr = 1e-3\n",
    "max_epochs = 250\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33317e8f-2a04-45ed-8e50-a847db9ce88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ca421-140c-4d6b-a5e2-003b23a0a278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "torch.manual_seed(0)\n",
    "\n",
    "data_df = pd.read_csv('./../Data/1000_features_survival_3classes.csv',index_col=0).drop(['index', 'y'],axis=1)\n",
    "data_df_event_time = data_df[['event', 'time']]\n",
    "\n",
    "\n",
    "data_df = pd.get_dummies(data_df.drop(['event', 'time'], axis=1),dtype='int')\n",
    "scaler = MinMaxScaler()\n",
    "data_df = pd.DataFrame(scaler.fit_transform(data_df), columns=data_df.columns)\n",
    "data_df['event'] = [int(e) for e in data_df_event_time['event']]\n",
    "data_df['time'] = data_df_event_time['time']\n",
    "\n",
    "data_df = data_df.fillna(data_df.mean())\n",
    "\n",
    "train_ci_ls = []\n",
    "valid_ci_ls = []\n",
    "test_ci_ls = []\n",
    "epoch_ls = []\n",
    "elapsed_time_ls = []\n",
    "nconcepts_ls = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"*******************\")\n",
    "    print(seed)\n",
    "    rules_df = pd.read_csv('./../results/RuleKit/rules_class1_2_seed'+str(seed)+'.csv',index_col=0)\n",
    "    conditions_ls_ls_ = rules_df.reset_index(drop=True)['conditions'].drop_duplicates().to_list()\n",
    "    \n",
    "    clinical_feature_ls = [feature for feature in data_df.columns.to_list() if 'EN' not in feature]\n",
    "    \n",
    "    conditions_ls_ls = []\n",
    "    for condition_ls in conditions_ls_ls_:\n",
    "        _ls = []\n",
    "        for condition in literal_eval(condition_ls):\n",
    "            if 'EN' in condition:\n",
    "                _ls = _ls + [condition]\n",
    "            else:\n",
    "                _ls = _ls + [clinical_feature for clinical_feature in clinical_feature_ls if condition in clinical_feature]\n",
    "                    \n",
    "        conditions_ls_ls = conditions_ls_ls + [_ls] \n",
    "    \n",
    "    feature_idx_dict = {feature:idx for idx,feature in enumerate(data_df.drop(['event', 'time'], axis=1).columns.to_list())}\n",
    "    feature_groups = [[feature_idx_dict[condition] for condition in condition_ls] for condition_ls in conditions_ls_ls]\n",
    "    nconcepts_ls = nconcepts_ls + [len(feature_groups)]\n",
    "\n",
    "    ##########3\n",
    "    data_train, data_tmp = train_test_split(data_df, test_size=test_size, random_state=seed)\n",
    "    data_val, data_test = train_test_split(data_tmp, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    X_train = torch.tensor(data_train.drop(['event', 'time'], axis=1).to_numpy(), dtype=torch.float32)\n",
    "    e_train = torch.tensor(data_train['event'].to_numpy(), dtype=torch.long)\n",
    "    t_train = torch.tensor(data_train['time'].to_numpy(), dtype=torch.long)\n",
    "    train_dataset = TensorDataset(X_train, e_train, t_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_val = torch.tensor(data_val.drop(['event', 'time'], axis=1).to_numpy(), dtype=torch.float32)\n",
    "    e_val = torch.tensor(data_val['event'].to_numpy(), dtype=torch.long)\n",
    "    t_val = torch.tensor(data_val['time'].to_numpy(), dtype=torch.long)\n",
    "    val_dataset = TensorDataset(X_val, e_val, t_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_test = torch.tensor(data_test.drop(['event', 'time'], axis=1).to_numpy(), dtype=torch.float32)\n",
    "    e_test = torch.tensor(data_test['event'].to_numpy(), dtype=torch.long)\n",
    "    t_test = torch.tensor(data_test['time'].to_numpy(), dtype=torch.long)\n",
    "    test_dataset = TensorDataset(X_test, e_test, t_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # parameters\n",
    "    input_size = X_train.shape[1]  # Number of RNA expression features\n",
    "    \n",
    "    model = RuleMLP(input_size, feature_groups)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Train the model\n",
    "    start = time.time()\n",
    "    epoch = train(model, optimizer, train_loader, val_loader, max_epochs, l2_reg, patience=50)\n",
    "    end = time.time()\n",
    "    elapsed_time = end-start\n",
    "    \n",
    "    print(\"Time elapsed: \", elapsed_time)\n",
    "    print(\"Number of epoch: \", epoch)\n",
    "    epoch_ls = epoch_ls + [epoch]\n",
    "    elapsed_time_ls = elapsed_time_ls + [elapsed_time]\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    ## Training\n",
    "    events_ls = []\n",
    "    times_ls = []\n",
    "    predicted_ls = []\n",
    "    \n",
    "    for i, (inputs, events, times) in enumerate(train_loader):\n",
    "        predicted_ls = predicted_ls + model(inputs).reshape(-1).tolist()\n",
    "        events_ls = events_ls + events.tolist()\n",
    "        times_ls = times_ls + times.tolist()\n",
    "    \n",
    "    \n",
    "    print(\"train CI lifelines: \", concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls))\n",
    "    train_ci_ls = train_ci_ls + [concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls)]\n",
    "    \n",
    "    ## Validation\n",
    "    events_ls = []\n",
    "    times_ls = []\n",
    "    predicted_ls = []\n",
    "    \n",
    "    for i, (inputs, events, times) in enumerate(val_loader):\n",
    "        predicted_ls = predicted_ls + model(inputs).reshape(-1).tolist()\n",
    "        events_ls = events_ls + events.tolist()\n",
    "        times_ls = times_ls + times.tolist()\n",
    "    \n",
    "    print(\"valid CI lifelines: \", concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                      events_ls))\n",
    "    valid_ci_ls = valid_ci_ls + [concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls)]\n",
    "    ## test\n",
    "    events_ls = []\n",
    "    times_ls = []\n",
    "    predicted_ls = []\n",
    "    \n",
    "    for i, (inputs, events, times) in enumerate(test_loader):\n",
    "        predicted_ls = predicted_ls + model(inputs).reshape(-1).tolist()\n",
    "        events_ls = events_ls + events.tolist()\n",
    "        times_ls = times_ls + times.tolist()\n",
    "    \n",
    "    sorted_indices = np.argsort(times_ls).tolist()\n",
    "    \n",
    "    print(\"test CI lifelines: \", concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls))\n",
    "    test_ci_ls = test_ci_ls + [concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls)]\n",
    "\n",
    "    torch.save(model, './../models/RuleMLP/catrulekit_1hot_seed'+str(seed)+'.pt')\n",
    "    dict_to_save = {'feature_groups_idx': feature_groups,\n",
    "                'weights': model.fc2.weight[0].detach().numpy(),\n",
    "                'feature_groups':[[condition for condition in condition_ls] for condition_ls in conditions_ls_ls]}\n",
    "\n",
    "    with open('./../models/RuleMLP/catrulekit_1hot_concept_weights_seed'+str(seed)+'.pkl','wb') as f:\n",
    "        pickle.dump(dict_to_save,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7e1424b-b8df-41e9-8123-d29dca1693fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuleMLP(\n",
      "  (intermitten_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=20, out_features=20, bias=True)\n",
      "    (2): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (3): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (4): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (5): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (6): Linear(in_features=19, out_features=19, bias=True)\n",
      "    (7-9): 3 x Linear(in_features=21, out_features=21, bias=True)\n",
      "    (10-11): 2 x Linear(in_features=22, out_features=22, bias=True)\n",
      "    (12): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (13): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (14): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (15): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (16-17): 2 x Linear(in_features=19, out_features=19, bias=True)\n",
      "    (18): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (19): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (20): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (21-22): 2 x Linear(in_features=22, out_features=22, bias=True)\n",
      "    (23): Linear(in_features=30, out_features=30, bias=True)\n",
      "    (24-25): 2 x Linear(in_features=13, out_features=13, bias=True)\n",
      "    (26): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (27-28): 2 x Linear(in_features=14, out_features=14, bias=True)\n",
      "    (29): Linear(in_features=27, out_features=27, bias=True)\n",
      "    (30): Linear(in_features=18, out_features=18, bias=True)\n",
      "    (31): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (32): Linear(in_features=31, out_features=31, bias=True)\n",
      "    (33): Linear(in_features=26, out_features=26, bias=True)\n",
      "    (34-35): 2 x Linear(in_features=17, out_features=17, bias=True)\n",
      "    (36): Linear(in_features=19, out_features=19, bias=True)\n",
      "    (37): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (38): Linear(in_features=22, out_features=22, bias=True)\n",
      "    (39): Linear(in_features=29, out_features=29, bias=True)\n",
      "    (40-41): 2 x Linear(in_features=17, out_features=17, bias=True)\n",
      "    (42): Linear(in_features=14, out_features=14, bias=True)\n",
      "    (43): Linear(in_features=17, out_features=17, bias=True)\n",
      "    (44-46): 3 x Linear(in_features=15, out_features=15, bias=True)\n",
      "    (47): Linear(in_features=19, out_features=19, bias=True)\n",
      "    (48): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (49): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (50): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (51): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (52-53): 2 x Linear(in_features=19, out_features=19, bias=True)\n",
      "    (54): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (55): Linear(in_features=21, out_features=21, bias=True)\n",
      "    (56): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (57): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (58): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (59): Linear(in_features=36, out_features=36, bias=True)\n",
      "    (60): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (61): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (62): Linear(in_features=39, out_features=39, bias=True)\n",
      "    (63): Linear(in_features=47, out_features=47, bias=True)\n",
      "    (64): Linear(in_features=43, out_features=43, bias=True)\n",
      "    (65): Linear(in_features=51, out_features=51, bias=True)\n",
      "    (66): Linear(in_features=36, out_features=36, bias=True)\n",
      "    (67): Linear(in_features=54, out_features=54, bias=True)\n",
      "    (68): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (69): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (70): Linear(in_features=41, out_features=41, bias=True)\n",
      "    (71): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (72): Linear(in_features=29, out_features=29, bias=True)\n",
      "    (73): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (74): Linear(in_features=31, out_features=31, bias=True)\n",
      "    (75): Linear(in_features=48, out_features=48, bias=True)\n",
      "    (76): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (77): Linear(in_features=39, out_features=39, bias=True)\n",
      "    (78): Linear(in_features=49, out_features=49, bias=True)\n",
      "    (79): Linear(in_features=26, out_features=26, bias=True)\n",
      "    (80-81): 2 x Linear(in_features=12, out_features=12, bias=True)\n",
      "    (82): Linear(in_features=17, out_features=17, bias=True)\n",
      "  )\n",
      "  (bc1): BatchNorm1d(2067, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (group_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=20, out_features=1, bias=True)\n",
      "    (2): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (3): Linear(in_features=20, out_features=1, bias=True)\n",
      "    (4): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (5): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (6): Linear(in_features=19, out_features=1, bias=True)\n",
      "    (7-9): 3 x Linear(in_features=21, out_features=1, bias=True)\n",
      "    (10-11): 2 x Linear(in_features=22, out_features=1, bias=True)\n",
      "    (12): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (13): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (14): Linear(in_features=20, out_features=1, bias=True)\n",
      "    (15): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (16-17): 2 x Linear(in_features=19, out_features=1, bias=True)\n",
      "    (18): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (19): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (20): Linear(in_features=34, out_features=1, bias=True)\n",
      "    (21-22): 2 x Linear(in_features=22, out_features=1, bias=True)\n",
      "    (23): Linear(in_features=30, out_features=1, bias=True)\n",
      "    (24-25): 2 x Linear(in_features=13, out_features=1, bias=True)\n",
      "    (26): Linear(in_features=15, out_features=1, bias=True)\n",
      "    (27-28): 2 x Linear(in_features=14, out_features=1, bias=True)\n",
      "    (29): Linear(in_features=27, out_features=1, bias=True)\n",
      "    (30): Linear(in_features=18, out_features=1, bias=True)\n",
      "    (31): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (32): Linear(in_features=31, out_features=1, bias=True)\n",
      "    (33): Linear(in_features=26, out_features=1, bias=True)\n",
      "    (34-35): 2 x Linear(in_features=17, out_features=1, bias=True)\n",
      "    (36): Linear(in_features=19, out_features=1, bias=True)\n",
      "    (37): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (38): Linear(in_features=22, out_features=1, bias=True)\n",
      "    (39): Linear(in_features=29, out_features=1, bias=True)\n",
      "    (40-41): 2 x Linear(in_features=17, out_features=1, bias=True)\n",
      "    (42): Linear(in_features=14, out_features=1, bias=True)\n",
      "    (43): Linear(in_features=17, out_features=1, bias=True)\n",
      "    (44-46): 3 x Linear(in_features=15, out_features=1, bias=True)\n",
      "    (47): Linear(in_features=19, out_features=1, bias=True)\n",
      "    (48): Linear(in_features=24, out_features=1, bias=True)\n",
      "    (49): Linear(in_features=35, out_features=1, bias=True)\n",
      "    (50): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (51): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (52-53): 2 x Linear(in_features=19, out_features=1, bias=True)\n",
      "    (54): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (55): Linear(in_features=21, out_features=1, bias=True)\n",
      "    (56): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (57): Linear(in_features=10, out_features=1, bias=True)\n",
      "    (58): Linear(in_features=23, out_features=1, bias=True)\n",
      "    (59): Linear(in_features=36, out_features=1, bias=True)\n",
      "    (60): Linear(in_features=24, out_features=1, bias=True)\n",
      "    (61): Linear(in_features=50, out_features=1, bias=True)\n",
      "    (62): Linear(in_features=39, out_features=1, bias=True)\n",
      "    (63): Linear(in_features=47, out_features=1, bias=True)\n",
      "    (64): Linear(in_features=43, out_features=1, bias=True)\n",
      "    (65): Linear(in_features=51, out_features=1, bias=True)\n",
      "    (66): Linear(in_features=36, out_features=1, bias=True)\n",
      "    (67): Linear(in_features=54, out_features=1, bias=True)\n",
      "    (68): Linear(in_features=35, out_features=1, bias=True)\n",
      "    (69): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (70): Linear(in_features=41, out_features=1, bias=True)\n",
      "    (71): Linear(in_features=25, out_features=1, bias=True)\n",
      "    (72): Linear(in_features=29, out_features=1, bias=True)\n",
      "    (73): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (74): Linear(in_features=31, out_features=1, bias=True)\n",
      "    (75): Linear(in_features=48, out_features=1, bias=True)\n",
      "    (76): Linear(in_features=40, out_features=1, bias=True)\n",
      "    (77): Linear(in_features=39, out_features=1, bias=True)\n",
      "    (78): Linear(in_features=49, out_features=1, bias=True)\n",
      "    (79): Linear(in_features=26, out_features=1, bias=True)\n",
      "    (80-81): 2 x Linear(in_features=12, out_features=1, bias=True)\n",
      "    (82): Linear(in_features=17, out_features=1, bias=True)\n",
      "  )\n",
      "  (bc2): BatchNorm1d(83, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=83, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d40cc5f-8ec9-4c64-aefc-bd296183851f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9253888645443332 Valid: 0.7245054932932629 Test: 0.6823901922974325 Epochs: 114.9 Elapsed time 114.6811257839203 nconcepts 67.9\n",
      "\n",
      "Train:  [0.9217123247833421, 0.9389419126477466, 0.9223477792955006, 0.938849987455238, 0.9386712456542017, 0.9084960820969731, 0.9239788440706047, 0.9223626407944869, 0.9196256157635468, 0.9189022128816912]\n",
      "\n",
      "Valid:  [0.6517615176151762, 0.6796116504854369, 0.7603801900950475, 0.7951012717852096, 0.7122270742358079, 0.7615006699419383, 0.7258522727272727, 0.7218895542248835, 0.7264719517848864, 0.7102587800369686]\n",
      "\n",
      "Test:  [0.7604832977967306, 0.6836601307189543, 0.680952380952381, 0.7096873083997548, 0.6794320137693631, 0.7164090368608799, 0.6440951229343007, 0.6936845660249915, 0.6652334152334153, 0.5902646502835539]\n",
      "\n",
      "Epoch:  [106, 149, 166, 134, 83, 102, 110, 62, 119, 118]\n",
      "\n",
      "Elapsed time:  [95.34932351112366, 170.12869024276733, 160.9313440322876, 122.75166344642639, 78.18098211288452, 94.35079145431519, 102.73288178443909, 59.227784633636475, 123.19704055786133, 139.9607560634613]\n",
      "\n",
      "nconcepts:  [63, 75, 68, 61, 65, 64, 63, 65, 72, 83]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\",np.mean(train_ci_ls), \"Valid:\",np.mean(valid_ci_ls), \"Test:\",np.mean(test_ci_ls),\n",
    "      \"Epochs:\",np.mean(epoch_ls), \"Elapsed time\", np.mean(elapsed_time_ls), \"nconcepts\", np.mean(nconcepts_ls))\n",
    "\n",
    "print(\"\\nTrain: \", train_ci_ls) \n",
    "print(\"\\nValid: \", valid_ci_ls)\n",
    "print(\"\\nTest: \", test_ci_ls)\n",
    "print(\"\\nEpoch: \", epoch_ls)\n",
    "print(\"\\nElapsed time: \", elapsed_time_ls)\n",
    "print(\"\\nnconcepts: \", nconcepts_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b9bc4-42e4-491b-bc56-72784c30a561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b544e5-084d-4883-a08c-71becc4cc206",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9302355-6dee-47ec-9054-cfb651660020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec70698-9a4d-4aad-b8a3-fce4789abcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d67a33-2416-4997-9873-9447dbcd746b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
