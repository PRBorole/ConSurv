{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe7c058-ce9a-4e9c-9fbc-b78a73a8e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import time \n",
    "import sys \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_tree\n",
    "import lifelines\n",
    "\n",
    "sys.path.append('./../src/')\n",
    "from utils import *\n",
    "from utils_xgboost import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e0586-44b2-4bce-8517-c34e84379dc2",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e31267f-2bda-4806-817f-d4bcc75664ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  353\n",
      "Number of rules:  12243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████████▊                                                                                                                                                       | 1/10 [01:20<12:00, 80.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  360\n",
      "Number of rules:  12580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████████████████████▌                                                                                                                                      | 2/10 [02:41<10:48, 81.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  381\n",
      "Number of rules:  13457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████████████████████████████████▍                                                                                                                     | 3/10 [04:11<09:55, 85.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  347\n",
      "Number of rules:  11913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████▏                                                                                                    | 4/10 [05:30<08:16, 82.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  332\n",
      "Number of rules:  12165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████████████                                                                                    | 5/10 [06:52<06:52, 82.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  352\n",
      "Number of rules:  12096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 6/10 [08:11<05:24, 81.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  387\n",
      "Number of rules:  13737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                  | 7/10 [09:41<04:12, 84.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  385\n",
      "Number of rules:  13222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 8/10 [11:06<02:48, 84.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  387\n",
      "Number of rules:  12800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 9/10 [12:28<01:23, 83.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees:  363\n",
      "Number of rules:  13142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [13:53<00:00, 83.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "data_df = pd.read_csv('./../Data/breast_cancer/1000_features_survival_3classes.csv',\n",
    "                      index_col=0).drop(['index'],axis=1)\n",
    "\n",
    "\n",
    "data_df_event_time = data_df[['event', 'time']]\n",
    "\n",
    "\n",
    "data_df = pd.get_dummies(data_df.drop(['event', 'time'], axis=1),dtype=int)\n",
    "scaler = MinMaxScaler()\n",
    "data_df = pd.DataFrame(scaler.fit_transform(data_df), columns=data_df.columns)\n",
    "data_df['event'] = [int(e) for e in data_df_event_time['event']]\n",
    "data_df['time'] = data_df_event_time['time']\n",
    "\n",
    "data_df = data_df.fillna(data_df.mean())\n",
    "\n",
    "val_ls = []\n",
    "test_ls = []\n",
    "elapsed_time_ls = []\n",
    "\n",
    "# Split data into training and validation sets\n",
    "test_size=0.3\n",
    "max_depth = 10\n",
    "\n",
    "params = {'verbosity': 0,\n",
    "              'objective': 'survival:aft',\n",
    "              'eval_metric': 'aft-nloglik',\n",
    "              'tree_method': 'hist',\n",
    "              'learning_rate': 0.01,\n",
    "              'aft_loss_distribution': 'logistic',\n",
    "              'aft_loss_distribution_scale': 1.2,\n",
    "              'max_depth': max_depth,\n",
    "              'lambda': 0.01,\n",
    "              'alpha': 0.1}\n",
    "\n",
    "num_boost_round = 500\n",
    "\n",
    "seeds = [999, 7, 42, 1995, 1303, 2405, 1996, 200, 0, 777]\n",
    "pruning_result_df = []\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    data_train, data_tmp = train_test_split(data_df, test_size=test_size, random_state=seed)\n",
    "    data_val, data_test = train_test_split(data_tmp, test_size=test_size, random_state=seed)\n",
    "    \n",
    "    X_train = data_train.drop(['event', 'time','y'], axis=1)\n",
    "    y_lower_train = data_train['time']\n",
    "    y_upper_train = np.array([t if e else np.inf for t,e in zip(data_train['time'], data_train['event'])])\n",
    "    dtrain = xgb.DMatrix(X_train.values)\n",
    "    dtrain.set_float_info('label_lower_bound', y_lower_train)\n",
    "    dtrain.set_float_info('label_upper_bound', y_upper_train)\n",
    "    \n",
    "    X_val = data_val.drop(['event', 'time','y'], axis=1)\n",
    "    y_lower_val = data_val['time']\n",
    "    y_upper_val = np.array([t if e else np.inf for t,e in zip(data_val['time'], data_val['event'])])\n",
    "    dvalid = xgb.DMatrix(X_val.values)\n",
    "    dvalid.set_float_info('label_lower_bound', y_lower_val)\n",
    "    dvalid.set_float_info('label_upper_bound', y_upper_val)\n",
    "    \n",
    "    X_test = data_test.drop(['event', 'time','y'], axis=1)\n",
    "    y_lower_test = data_test['time']\n",
    "    y_upper_test = np.array([t if e else np.inf for t,e in zip(data_test['time'], data_test['event'])])\n",
    "    dtest = xgb.DMatrix(X_test.values)\n",
    "    dtest.set_float_info('label_lower_bound', y_lower_test)\n",
    "    dtest.set_float_info('label_upper_bound', y_upper_test)\n",
    "    \n",
    "    \n",
    "    bst = xgb.train(params, dtrain, num_boost_round=num_boost_round,\n",
    "                    evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                    early_stopping_rounds=50, verbose_eval=False)\n",
    "    \n",
    "    \n",
    "    xbgboost_rules = [] \n",
    "    ntree = 0\n",
    "    max_depth_rule = []\n",
    "    \n",
    "    for tree_str in bst.get_dump():\n",
    "        if_then_rules = parse_tree(tree_str)\n",
    "        ntree = ntree+1\n",
    "        xbgboost_rules.extend(if_then_rules)\n",
    "        \n",
    "    print(\"Number of trees: \", ntree)\n",
    "    print(\"Number of rules: \", len(xbgboost_rules))\n",
    "\n",
    "    pruning_depths = [i+1 for i in range(max_depth)]\n",
    "    pruning_depths = [5]\n",
    "    ntree_ls = []\n",
    "    nxbgboost_rules_ls = []\n",
    "    c_index_val = []\n",
    "    c_index_test = []\n",
    "\n",
    "    \n",
    "    for pruning_depth in pruning_depths:\n",
    "        rules_df = {'rules':[], 'seed':[], 'pruning_depth':[]}\n",
    "        pruned = xgb.train(\n",
    "                {\"process_type\": \"update\", \"updater\": \"prune\", \"max_depth\": pruning_depth},\n",
    "                dtrain,\n",
    "                num_boost_round=len(bst.get_dump()),\n",
    "                xgb_model=bst,\n",
    "                evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                verbose_eval=False\n",
    "            )\n",
    "        \n",
    "        xbgboost_rules = [] \n",
    "        tree_idx_ls = []\n",
    "        ntree = 0\n",
    "        for tree_str in pruned.get_dump():\n",
    "            if_then_rules = parse_tree(tree_str)\n",
    "            tree_idx_ls = tree_idx_ls + [ntree]*len(if_then_rules)\n",
    "            ntree = ntree+1\n",
    "            xbgboost_rules.extend(if_then_rules)\n",
    "            \n",
    "            \n",
    "        ntree_ls = ntree_ls + [ntree]\n",
    "        nxbgboost_rules_ls = nxbgboost_rules_ls + [len(xbgboost_rules)]\n",
    "\n",
    "        rules_df['pruning_depth'] = rules_df['pruning_depth'] + [pruning_depth]*len(xbgboost_rules)\n",
    "        rules_df['seed'] = rules_df['seed'] + [seed]*len(xbgboost_rules)\n",
    "        rules_df['rules'] = rules_df['rules'] + xbgboost_rules\n",
    "        rules_df = pd.DataFrame(rules_df)\n",
    "        rules_df['conditions'] = [[condition[1:].split('=')[0]\n",
    "                                    for condition in re.sub('>','',re.sub('<','=', rule)).split()\n",
    "                                      if '=' in condition] \n",
    "                                    for rule in rules_df['rules']]\n",
    "        rules_df['nconditions'] = [len([condition[1:].split('=')[0]\n",
    "                                        for condition in re.sub('>','',re.sub('<','=', rule)).split()\n",
    "                                          if '=' in condition])\n",
    "                                        for rule in rules_df['rules']]\n",
    "        rules_df['tree_idx'] = tree_idx_ls\n",
    "        rules_df.to_csv('./../results/XGBoost/rules_seed'+str(seed)+'_pruning_depth_'+str(pruning_depth)+'.csv')\n",
    "        \n",
    "        # Run prediction on the validation set\n",
    "        df = pd.DataFrame({'Label (lower bound)': y_lower_val,\n",
    "                           'Label (upper bound)': y_upper_val,\n",
    "                           'Predicted label': pruned.predict(dvalid)})\n",
    "        \n",
    "        c_index_val = c_index_val + [lifelines.utils.concordance_index(event_times = data_val['time'], \n",
    "                                                                          predicted_scores = df['Predicted label'], \n",
    "                                                                          event_observed = data_val['event'])]\n",
    "    \n",
    "    \n",
    "        # Run prediction on the test set\n",
    "        df = pd.DataFrame({'Label (lower bound)': y_lower_test,\n",
    "                           'Label (upper bound)': y_upper_test,\n",
    "                           'Predicted label': pruned.predict(dtest)})\n",
    "        \n",
    "        c_index_test = c_index_test + [lifelines.utils.concordance_index(event_times = data_test['time'], \n",
    "                                                                          predicted_scores = df['Predicted label'], \n",
    "                                                                          event_observed = data_test['event'])]\n",
    "        \n",
    "    pruning_result_df = pruning_result_df + [pd.DataFrame({'pruning_depths':pruning_depths,\n",
    "                                                        'ntree': ntree_ls, \n",
    "                                                        'xbgboost_rules':nxbgboost_rules_ls,\n",
    "                                                        'c_index_val': c_index_val,\n",
    "                                                        'c_index_test': c_index_test,\n",
    "                                                        'seed': [seed]*len(pruning_depths)})]\n",
    "\n",
    "\n",
    "pruning_result_df = pd.concat(pruning_result_df)\n",
    "pruning_result_df.to_csv('./../results/XGBoost/survival_tree_prunning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63da004-e5ca-4ac4-9c8c-eb8418593176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
