{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "917a93f1-a62d-491b-ad19-e47e8c96edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import json\n",
    "from ast import literal_eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "\n",
    "from torchsurv.loss import cox\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "sys.path.append('./../src/')\n",
    "from utils import *\n",
    "from utils_XGBMLP import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255338b3-3cdb-40a8-81a8-b871f6e6176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "seeds = [999, 7, 42, 1995, 1303, 2405, 1996, 200, 0, 777]\n",
    "test_size = 0.3\n",
    "batch_size = 64\n",
    "\n",
    "l2_reg = 0.001\n",
    "lr = 1e-3\n",
    "max_epochs = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d41336-621f-4e49-852f-fc24be1e064c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b824c0c-06be-4c04-b699-b537bf04099a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "torch.manual_seed(0)\n",
    "\n",
    "data_df = pd.read_csv('./../Data/1000_features_survival_3classes.csv',index_col=0).drop(['index', 'y'],axis=1)\n",
    "data_df_event_time = data_df[['event', 'time']]\n",
    "\n",
    "\n",
    "data_df = pd.get_dummies(data_df.drop(['event', 'time'], axis=1),dtype='int')\n",
    "scaler = MinMaxScaler()\n",
    "data_df = pd.DataFrame(scaler.fit_transform(data_df), columns=data_df.columns)\n",
    "data_df['event'] = [int(e) for e in data_df_event_time['event']]\n",
    "data_df['time'] = data_df_event_time['time']\n",
    "\n",
    "data_df = data_df.fillna(data_df.mean())\n",
    "\n",
    "\n",
    "train_ci_ls = []\n",
    "valid_ci_ls = []\n",
    "test_ci_ls = []\n",
    "epoch_ls = []\n",
    "elapsed_time_ls = []\n",
    "nconcepts_ls = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"*******************\")\n",
    "    print(seed)\n",
    "    rules_df = pd.read_csv('./../results/XGBoost/rules_seed'+str(seed)+'_pruning_depth_5.csv',index_col=0)\n",
    "    \n",
    "    data_train, data_tmp = train_test_split(data_df, test_size=test_size, random_state=seed)\n",
    "    data_val, data_test = train_test_split(data_tmp, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    X_train = torch.tensor(data_train.drop(['event', 'time'], axis=1).to_numpy(), dtype=torch.float32)\n",
    "    e_train = torch.tensor(data_train['event'].to_numpy(), dtype=torch.long)\n",
    "    t_train = torch.tensor(data_train['time'].to_numpy(), dtype=torch.long)\n",
    "    train_dataset = TensorDataset(X_train, e_train, t_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_val = torch.tensor(data_val.drop(['event', 'time'], axis=1).to_numpy(), dtype=torch.float32)\n",
    "    e_val = torch.tensor(data_val['event'].to_numpy(), dtype=torch.long)\n",
    "    t_val = torch.tensor(data_val['time'].to_numpy(), dtype=torch.long)\n",
    "    val_dataset = TensorDataset(X_val, e_val, t_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_test = torch.tensor(data_test.drop(['event', 'time'], axis=1).to_numpy(), dtype=torch.float32)\n",
    "    e_test = torch.tensor(data_test['event'].to_numpy(), dtype=torch.long)\n",
    "    t_test = torch.tensor(data_test['time'].to_numpy(), dtype=torch.long)\n",
    "    test_dataset = TensorDataset(X_test, e_test, t_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # create dict of feature index so groups can be passed to MLP\n",
    "    feature_groups = [[int(condition[1:]) for condition in literal_eval(conditions)] for conditions in rules_df['conditions']]\n",
    "    feature_group_ls = []\n",
    "    for feature in feature_groups:\n",
    "        if list(set(feature)) not in feature_group_ls:\n",
    "            feature_group_ls = feature_group_ls + [list(set(feature))]\n",
    "\n",
    "    \n",
    "    print(len(feature_group_ls))\n",
    "\n",
    "    nconcepts_ls = nconcepts_ls + [len(feature_group_ls)]\n",
    "    \n",
    "    # parameters\n",
    "    input_size = X_train.shape[1]  # Number of RNA expression features\n",
    "    \n",
    "    model = XGBMLP(input_size, feature_group_ls)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Train the model, \n",
    "    start = time.time()\n",
    "    print(\"Starting Training\")\n",
    "    epoch = train(model, optimizer, train_loader, val_loader, max_epochs, l2_reg)\n",
    "    end = time.time()\n",
    "    elapsed_time = end-start\n",
    "    \n",
    "    print(\"Time elapsed: \", elapsed_time)\n",
    "    print(\"Number of epoch: \", epoch)\n",
    "    epoch_ls = epoch_ls + [epoch]\n",
    "    elapsed_time_ls = elapsed_time_ls + [elapsed_time]\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    ## Training\n",
    "    events_ls = []\n",
    "    times_ls = []\n",
    "    predicted_ls = []\n",
    "    \n",
    "    for i, (inputs, events, times) in enumerate(train_loader):\n",
    "        predicted_ls = predicted_ls + model(inputs).reshape(-1).tolist()\n",
    "        events_ls = events_ls + events.tolist()\n",
    "        times_ls = times_ls + times.tolist()\n",
    "    \n",
    "    \n",
    "    print(\"train CI lifelines: \", concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls))\n",
    "    train_ci_ls = train_ci_ls + [concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls)]\n",
    "    \n",
    "    ## Validation\n",
    "    events_ls = []\n",
    "    times_ls = []\n",
    "    predicted_ls = []\n",
    "    \n",
    "    for i, (inputs, events, times) in enumerate(val_loader):\n",
    "        predicted_ls = predicted_ls + model(inputs).reshape(-1).tolist()\n",
    "        events_ls = events_ls + events.tolist()\n",
    "        times_ls = times_ls + times.tolist()\n",
    "    \n",
    "    print(\"valid CI lifelines: \", concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls))\n",
    "    valid_ci_ls = valid_ci_ls + [concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls)]\n",
    "    ## test\n",
    "    events_ls = []\n",
    "    times_ls = []\n",
    "    predicted_ls = []\n",
    "    \n",
    "    for i, (inputs, events, times) in enumerate(test_loader):\n",
    "        predicted_ls = predicted_ls + model(inputs).reshape(-1).tolist()\n",
    "        events_ls = events_ls + events.tolist()\n",
    "        times_ls = times_ls + times.tolist()\n",
    "    \n",
    "    sorted_indices = np.argsort(times_ls).tolist()\n",
    "    \n",
    "    print(\"test CI lifelines: \", concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls))\n",
    "    test_ci_ls = test_ci_ls + [concordance_index(times_ls, \n",
    "                                                     [-i for i in predicted_ls],  \n",
    "                                                     events_ls)]\n",
    "\n",
    "    torch.save(model, './../models/XGBMLP/XGBMLP_seed'+str(seed)+'.pt')\n",
    "    dict_to_save = {'feature_groups_idx': feature_group_ls,\n",
    "                'weights': model.fc2.weight[0].detach().numpy(),\n",
    "                'feature_groups':[[data_train.columns[feature] for feature in feature_group] for feature_group in feature_group_ls]}\n",
    "\n",
    "    with open('./../models/XGBMLP/concept_weights_seed'+str(seed)+'.pkl','wb') as f:\n",
    "        pickle.dump(dict_to_save,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6587d565-f1cf-41fe-bb16-9aafb6984e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9000068678593971 Valid: 0.6878435659875906 Test: 0.6879466165347004 Epochs: 184.2 Elapsed time: 1529.5388562202454 nconcepts: 656.1\n",
      "\n",
      "Train:  [0.8835766494115822, 0.8899563870804627, 0.8883438884059771, 0.9246630020755879, 0.9128932417535827, 0.9108229585139811, 0.9033539369995115, 0.9137848472861277, 0.8953300492610837, 0.8773437178060741]\n",
      "\n",
      "Valid:  [0.5934959349593496, 0.5834068843777581, 0.7593796898449224, 0.6905322656617994, 0.7786026200873363, 0.6721750781598929, 0.6827651515151515, 0.6254158349966733, 0.7315716272600834, 0.761090573012939]\n",
      "\n",
      "Test:  [0.697228144989339, 0.6684095860566449, 0.645021645021645, 0.7121397915389331, 0.6437177280550774, 0.8317479191438764, 0.7230955259975816, 0.7443431273218507, 0.6953316953316954, 0.5184310018903592]\n",
      "\n",
      "Epoch:  [228, 194, 136, 133, 227, 196, 145, 166, 273, 144]\n",
      "\n",
      "Time:  [1445.7396335601807, 1548.035896062851, 886.264410495758, 1223.123381137848, 1725.9595630168915, 1709.6599242687225, 1532.4133903980255, 1629.3712539672852, 2419.709561109543, 1175.1115481853485]\n",
      "\n",
      "nconcepts:  [552, 671, 544, 705, 602, 671, 771, 736, 673, 636]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\",np.mean(train_ci_ls), \"Valid:\",np.mean(valid_ci_ls), \"Test:\",np.mean(test_ci_ls),\n",
    "      \"Epochs:\",np.mean(epoch_ls), \"Elapsed time:\", np.mean(elapsed_time_ls), \"nconcepts:\",np.mean(nconcepts_ls))\n",
    "\n",
    "print(\"\\nTrain: \",train_ci_ls)\n",
    "print(\"\\nValid: \",valid_ci_ls)\n",
    "print(\"\\nTest: \",test_ci_ls)\n",
    "print(\"\\nEpoch: \",epoch_ls)\n",
    "print(\"\\nTime: \",elapsed_time_ls)\n",
    "print(\"\\nnconcepts: \",nconcepts_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdab74-184f-43c9-9151-93c102261506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9302355-6dee-47ec-9054-cfb651660020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458bbbf-4172-4095-9fe1-f92d9c26eb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba777b-cb97-46cc-aa53-0b9e8b8e38c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
